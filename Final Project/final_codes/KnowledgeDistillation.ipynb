{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNyLq8GC1yuxY5a2nG3mLH9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ldjropr_hZw9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670601189814,"user_tz":300,"elapsed":25209,"user":{"displayName":"Matthew Teichman","userId":"07203467577839204413"}},"outputId":"c4512302-c7ba-486b-88af-0c768cce70ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!git clone https://github.com/wonbeomjang/yolov5-knowledge-distillation\n","#!unzip \"/content/drive/MyDrive/Colab Code/RobotPerception/Project/TrafficSign/yolo_dataset.zip\" -d \"/content/dataset\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gmDbOb4hozwP","executionInfo":{"status":"ok","timestamp":1670601193677,"user_tz":300,"elapsed":3875,"user":{"displayName":"Matthew Teichman","userId":"07203467577839204413"}},"outputId":"53362430-851a-4958-f6d9-4bea6ad1b7a7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5-knowledge-distillation'...\n","remote: Enumerating objects: 14108, done.\u001b[K\n","remote: Total 14108 (delta 0), reused 0 (delta 0), pack-reused 14108\u001b[K\n","Receiving objects: 100% (14108/14108), 13.53 MiB | 23.10 MiB/s, done.\n","Resolving deltas: 100% (9592/9592), done.\n"]}]},{"cell_type":"code","source":["%cd yolov5-knowledge-distillation\n","%pip install -qr requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDm_1Gh2tBA2","executionInfo":{"status":"ok","timestamp":1670102872644,"user_tz":300,"elapsed":4854,"user":{"displayName":"Matthew Teichman","userId":"07203467577839204413"}},"outputId":"762ab7c7-badd-45c8-d571-540cba88d2b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'yolov5-knowledge-distillation'\n","/content/yolov5-knowledge-distillation\n"]}]},{"cell_type":"code","source":["python train.py --data data/traffic_data.yaml --hyp hyp.scratch-low.yaml -cfg yolov5s.yaml --weights yolov5s.pt --epoch=30 --batch-size 8 --teacher_weight yolov5m.pt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H59ZprOVtXCP","executionInfo":{"status":"ok","timestamp":1670102781055,"user_tz":300,"elapsed":882038,"user":{"displayName":"Matthew Teichman","userId":"07203467577839204413"}},"outputId":"1b78031b-3f9a-4d05-b471-cf917c0f87db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cur dir /content/yolov5-knowledge-distillation\n","\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, teacher_weight=yolov5m.pt, cfg=yolov5s.yaml, data=data/traffic_data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=30, batch_size=64, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","remote: Enumerating objects: 5, done.\u001b[K\n","remote: Counting objects: 100% (5/5), done.\u001b[K\n","remote: Total 5 (delta 4), reused 5 (delta 4), pack-reused 0\u001b[K\n","Unpacking objects: 100% (5/5), done.\n","From https://github.com/ultralytics/yolov5\n","   41c797cd..9db83afb  segment_cleanup -> ultralytics/segment_cleanup\n","\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 84 commits. Use `git pull ultralytics master` or `git clone https://github.com/ultralytics/yolov5` to update.\n","YOLOv5 üöÄ v6.2-237-g6660e81b Python-3.8.15 torch-1.12.1+cu113 CPU\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","Overriding model.yaml nc=80 with nc=5\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     26970  models.yolo.Detect                      [5, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","YOLOv5s summary: 214 layers, 7033114 parameters, 7033114 gradients, 16.0 GFLOPs\n","\n","Transferred 342/349 items from yolov5s.pt\n","Overriding model.yaml nc=80 with nc=5\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     26970  models.yolo.Detect                      [5, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","YOLOv5s summary: 214 layers, 7033114 parameters, 7033114 gradients, 16.0 GFLOPs\n","\n","Load teacher model from yolov5m.pt\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/dataset/content/yolo_dataset/traffic_light/labels/train.cache' images and labels... 12249 found, 0 missing, 344 empty, 21 corrupt: 100% 12249/12249 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-17_14-30-02-770890_k0.png: ignoring corrupt image/label: negative label values [     -0.007]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-17_14-39-19-898809_k0.png: ignoring corrupt image/label: negative label values [     -0.043]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-21_17-13-40-151647_k0.png: ignoring corrupt image/label: negative label values [      -0.01]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-22_12-33-17-956855_k0.png: ignoring corrupt image/label: negative label values [     -0.005]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-22_12-33-18-425720_k0.png: ignoring corrupt image/label: negative label values [     -0.024]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-22_13-18-12-271532_k0.png: ignoring corrupt image/label: negative label values [     -0.003]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-22_14-52-34-034554_k0.png: ignoring corrupt image/label: negative label values [     -0.018]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-22_14-54-52-832842_k0.png: ignoring corrupt image/label: negative label values [     -0.005]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-22_14-56-19-566378_k0.png: ignoring corrupt image/label: negative label values [     -0.023]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-22_15-04-39-764296_k0.png: ignoring corrupt image/label: negative label values [     -0.001]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-24_12-50-05-957421_k0.png: ignoring corrupt image/label: negative label values [     -0.007]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-24_13-40-55-714415_k0.png: ignoring corrupt image/label: negative label values [     -0.007]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-24_13-48-03-294392_k0.png: ignoring corrupt image/label: negative label values [      -0.02]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-24_17-52-10-479401_k0.png: ignoring corrupt image/label: negative label values [     -0.021]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-24_18-01-52-684964_k0.png: ignoring corrupt image/label: negative label values [     -0.037]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-24_18-08-41-093262_k0.png: ignoring corrupt image/label: negative label values [     -0.006]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-24_18-10-01-963549_k0.png: ignoring corrupt image/label: negative label values [     -0.043]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-24_18-19-00-293733_k0.png: ignoring corrupt image/label: negative label values [     -0.002]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-24_18-20-20-518385_k0.png: ignoring corrupt image/label: negative label values [     -0.009      -0.013]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-24_18-38-09-647834_k0.png: ignoring corrupt image/label: negative label values [     -0.015]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/dataset/content/yolo_dataset/traffic_light/images/train/DE_BBBR667_2015-04-24_18-40-09-215404_k0.png: ignoring corrupt image/label: negative label values [     -0.022]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/dataset/content/yolo_dataset/traffic_light/labels/valid.cache' images and labels... 400 found, 0 missing, 14 empty, 0 corrupt: 100% 400/400 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m1.33 anchors/target, 0.724 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING ‚ö†Ô∏è Extremely small objects found: 16918 of 44579 labels are <3 pixels in size\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 44579 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.8612: 100% 1000/1000 [00:38<00:00, 26.04it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 7.96 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.520/0.861-mean/best, past_thr=0.563-mean: 2,7, 3,9, 3,11, 4,9, 4,14, 5,17, 7,21, 9,29, 13,39\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/train/exp3/labels.jpg... \n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/train/exp3\u001b[0m\n","Starting training for 30 epochs...\n","/usr/local/lib/python3.8/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","  0% 0/192 [00:00<?, ?it/s]Process Process-3:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.8/threading.py\", line 1011, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n","    elif lock.acquire(block, timeout):\n","KeyboardInterrupt\n","^C\n"]}]}]}